# è¯„ä»·æµ + å•ç¨‹ç”Ÿæˆæµ   âš  ç»ˆæžçŽ‹ç‚¸ç‰ˆï¼ðŸ’£ðŸ’£ðŸ’£
# ---- ðŸ‘‡ å®žçŽ°é€»è¾‘ ----
"""
æµçº¿ 1ï¼šç”Ÿæˆåˆ‡ç‰‡ -> å¾—åˆ°è¯„ä»·ç»“æžœ(ä¸é€šè¿‡) -> ç”Ÿæˆåˆ‡ç‰‡ -> ... -> å¾—åˆ°è¯„ä»·ç»“æžœ(é€šè¿‡)
æµçº¿ 2ï¼šè¯„ä»·åˆ‡ç‰‡
"""
# ---- ðŸŽˆ å¯¼å…¥åº“  ----
import json
import time
import pandas as pd
from google import genai
from google.genai.types import Content, Part
import concurrent.futures

# ---- ðŸ“‚ æ–‡ä»¶å¤¹ ----
mydir = r"C:\Users"     #æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹
# ---- ðŸ”‘ API Key ----
client = genai.Client(api_key="Your API Key")   #æ›¿æ¢ä¸ºä½ çš„API Key
# ---- ðŸ“Œ å®šä¹‰å‡½æ•° ----
# âœ…è¯»å–prompt.txt
def read_prompt(file_name: str) -> str:
    with open(file_name, "r", encoding="utf-8") as f:
        return f.read().strip()
# âœ…è°ƒç”¨gemini2.5pro
def quest_gemini_25(prompt_list, max_retries=10, retry_delay=3):
    start_time = time.time()
    attempt = 0
    while attempt < max_retries:
        try:
            # Non-streaming:
            response = client.models.generate_content(
                    model="gemini-2.5-pro",
                    contents=prompt_list
            )
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"Execution time: {execution_time} seconds")
            answer = response.text
            # print(reply)
            # think = completion.choices[0].message.reasoning_content
            return  answer

        except Exception as e:
            attempt += 1
            print(f"Error occurred (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                print(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
            else:
                print("Max retries reached. Aborting.")
                return None
    return None
# âœ…è°ƒç”¨2.5proå¤„ç†å½“å‰åœºæ™¯ä»»åŠ¡
def generate_manage(prompt_list,max_attempts=5):
    print("è°ƒç”¨ gemini2.5pro ...")
    for attempt in range(max_attempts + 1):  # åŒ…æ‹¬åˆå§‹å°è¯•
        feedback = quest_gemini_25(prompt_list)  #
        try:
            # å°è¯•è§£æž JSON æ ¼å¼çš„åé¦ˆ
            json.loads(feedback)
            return feedback
        except json.JSONDecodeError:
            if attempt < max_attempts:
                continue  # é‡è¯•
            else:
                print("è°ƒç”¨2.5proå¤±è´¥")
                return "è§£æžé”™è¯¯"
    return None

# ---- ðŸ“Œ å®šä¹‰promptå˜é‡ ----
# ðŸ’»ç”Ÿæˆåˆ‡ç‰‡, è¿”å›žðŸ‘‰ [{{"åˆ‡ç‰‡åºå·":"","å¼€å§‹ID":"","ç»“æŸID":"", "ä¸»é¢˜å†…å®¹":""}}...]
prompt_cut = read_prompt(mydir+"\\"+"your_txt_1.txt")   #æ›¿æ¢ä¸ºä½ çš„æµçº¿1promptæ–‡æœ¬æ–‡ä»¶
# ðŸ’»è¯„ä»·åˆ‡ç‰‡, è¿”å›žðŸ‘‰ {{"æ˜¯å¦é€šè¿‡": "é€šè¿‡","åŽŸå› ": "ä½ çš„åŽŸå› è¯´æ˜Ž"}}
prompt_eval = read_prompt(mydir+"\\"+"your_txt_2.txt")  ##æ›¿æ¢ä¸ºä½ çš„æµçº¿2promptæ–‡æœ¬æ–‡ä»¶

# ---- ðŸ“Œ å¹¶å‘å¤„ç†çš„æ ¸å¿ƒå‡½æ•° ----
def process_row(row):
    # æµçº¿1çš„åŽ†å²è®°å½•, èµ‹ç©ºåˆ—è¡¨
    prompt_n_history = []
    history = row["å¯¹è¯å†…å®¹"]
# ---- ðŸŽ¢ æµçº¿ 1 ----
    print("ç”Ÿæˆç»“æžœ")
    prompt_n_history.append(Content(role="user", parts=[Part(
        text=prompt_cut.format(history=history))]))
    reply = generate_manage(prompt_n_history)
    prompt_n_history.append(Content(role="model", parts=[Part(text=reply)]))
# ---- ðŸŽ¢ æµçº¿ 2 ----
    print("ç¬¬1æ¬¡è¯„ä»·")
    prompt_eval_list = [Content(role="user", parts=[Part(
        text=prompt_eval.format(history=history,segment_rules=prompt_cut,segments=reply))])]
    evaluation = generate_manage(prompt_eval_list)
# ---- è¿”å›žæµçº¿ 1 è¿›è¡Œè¯„ä»·åŽåˆ‡ç‰‡ç”Ÿæˆï¼Œåˆ‡ç‰‡1ã€2å¾ªçŽ¯ ----
    max_opt = 1
    while json.loads(evaluation).get("æ˜¯å¦é€šè¿‡") == "ä¸é€šè¿‡" and max_opt < 16:
        print("è¯„ä»·æœªé€šè¿‡ï¼Œå†æ¬¡ç”Ÿæˆç»“æžœ")
        prompt_n_history.append(Content(role="user", parts=[Part(
            text="å¯¹ä½ çš„ç”Ÿæˆç»“æžœçš„è¯„ä»·æ˜¯:"+evaluation+"\næ®æ­¤è¯„ä»·ï¼Œä¼˜åŒ–ä½ çš„ç­”æ¡ˆï¼Œä¿æŒå’Œä¸Šä¸€è½®è¾“å‡ºæ ¼å¼ä¸€è‡´")]))
        reply = generate_manage(prompt_n_history)
        print(f"ç¬¬{max_opt+1}æ¬¡è¯„ä»·")
        prompt_eval_list = [Content(role="user", parts=[Part(
            text=prompt_eval.format(history=history, segment_rules=prompt_cut, segments=reply))])]
        evaluation = generate_manage(prompt_eval_list)
        max_opt = max_opt + 1
    return reply,max_opt
df = pd.read_excel(mydir + "\\" + "your_excel.xlsx")    #æ›¿æ¢ä¸ºä½ çš„xlsxæˆ–å…¶ä»–æ–‡ä»¶

# ---- çº¿ç¨‹æ± å¹¶å‘â—â—â— ----
rows = [row for _, row in df.iterrows()]  # æå‰å±•å¼€
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    results = list(executor.map(process_row, rows))
df["æ¨¡åž‹ç”Ÿæˆç»“æžœ"] = [r[0] for r in results]
df["ç»è¿‡è¯„ä»·æ¬¡æ•°"] = [r[1] for r in results]
df.to_excel(mydir + "\\" + "åŒæµçº¿ç»“æžœ.xlsx")

